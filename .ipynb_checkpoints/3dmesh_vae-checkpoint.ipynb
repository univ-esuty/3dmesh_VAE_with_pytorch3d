{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7d267e",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(''))\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"device:{device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ae9f7",
   "metadata": {},
   "source": [
    "## set train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VIEWS = 15\n",
    "Z_DIM = 100\n",
    "Lr = 1e-4\n",
    "MAX_ITER = 100000\n",
    "SAVE_ITER = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad597c",
   "metadata": {},
   "source": [
    "## create renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe53a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.utils import ico_sphere\n",
    "\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, save_obj, IO\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    OpenGLPerspectiveCameras, \n",
    "    PointLights, \n",
    "    DirectionalLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    SoftPhongShader,\n",
    "    TexturesVertex\n",
    ")\n",
    "\n",
    "from plot_image_grid import image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86228300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Render_silhouette():\n",
    "    def __init__(self, num_views, device, image_size=64):\n",
    "        # the number of different viewpoints from which we want to render the mesh.\n",
    "        self.num_views = num_views\n",
    "        self.device = device\n",
    "\n",
    "        # # Get a batch of viewing angles. \n",
    "        elev = torch.linspace(0, 360, self.num_views)\n",
    "        azim = torch.linspace(-180, 180, self.num_views)\n",
    "\n",
    "        # Place a point light in front of the object. As mentioned above, the front of \n",
    "        self.lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "        # Initialize an OpenGL perspective camera that represents a batch of different \n",
    "        # viewing angles. All the cameras helper methods support mixed type inputs and \n",
    "        # broadcasting. So we can view the camera from the a distance of dist=2.7, and \n",
    "        # then specify elevation and azimuth angles for each viewpoint as tensors. \n",
    "        R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim) \n",
    "        self.cameras = OpenGLPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "        # We arbitrarily choose one particular view that will be used to visualize \n",
    "        # results\n",
    "        camera = OpenGLPerspectiveCameras(device=device, R=R[None, 1, ...], \n",
    "                                          T=T[None, 1, ...]) \n",
    "\n",
    "        # Rasterization settings for silhouette rendering  \n",
    "        sigma = 1e-4\n",
    "        raster_settings_silhouette = RasterizationSettings(\n",
    "            image_size=image_size, \n",
    "            blur_radius=np.log(1. / 1e-4 - 1.)*sigma, \n",
    "            faces_per_pixel=50,\n",
    "            perspective_correct=False  ## avoid nan in backprop\n",
    "        )\n",
    "\n",
    "        # Silhouette renderer \n",
    "        self.renderer_silhouette = MeshRenderer(\n",
    "            rasterizer=MeshRasterizer(\n",
    "                cameras=camera, \n",
    "                raster_settings=raster_settings_silhouette\n",
    "            ),\n",
    "            shader=SoftSilhouetteShader()\n",
    "        )\n",
    "        \n",
    "    def render_silhouette(self, mesh):\n",
    "        # We scale normalize and center the target mesh to fit in a sphere of radius 1 \n",
    "        # centered at (0,0,0). (scale, center) will be used to bring the predicted mesh \n",
    "        # to its original center and scale.  Note that normalizing the target mesh, \n",
    "        # speeds up the optimization but is not necessary!\n",
    "        verts = mesh.verts_packed()\n",
    "        N = verts.shape[0]\n",
    "        center = verts.mean(0)\n",
    "        scale = max((verts - center).abs().max(0)[0])\n",
    "        mesh.offset_verts_(-center)\n",
    "        mesh.scale_verts_((1.0 / float(scale)));\n",
    "        \n",
    "        # Create a batch of meshes by repeating the cow mesh and associated textures. \n",
    "        # Meshes has a useful `extend` method which allows us do this very easily. \n",
    "        # This also extends the textures. \n",
    "        meshes = mesh.extend(self.num_views)\n",
    "\n",
    "        # Render silhouette images.  The 3rd channel of the rendering output is \n",
    "        # the alpha/silhouette channel\n",
    "        silhouette_images = self.renderer_silhouette(meshes, cameras=self.cameras, lights=self.lights)[..., 3]\n",
    "        return torch.clamp(silhouette_images.unsqueeze(dim=0), min=0.001, max=0.999) # avoid nan in backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST rendering\n",
    "io = IO()\n",
    "mesh = io.load_mesh('modelnet/off/chair_0001.off', device=device, load_textures=True)\n",
    "\n",
    "render_s = Render_silhouette(15, device, image_size=64)\n",
    "res = render_s.render_silhouette(mesh)\n",
    "\n",
    "print(mesh.textures)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for x in range(3*5):\n",
    "    plt.subplot(3, 5, x+1)\n",
    "    plt.imshow(res[0, x].cpu().detach().numpy())\n",
    "    plt.axis('off')\n",
    "plt.savefig('result/modelnet/sample.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4f4dc",
   "metadata": {},
   "source": [
    "## create trainable autoencoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb775af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VarAutoEncoder(nn.Module):\n",
    "    def __init__(self, z_dim, num_views, render_s):\n",
    "        super(VarAutoEncoder, self).__init__()\n",
    "        \n",
    "        self.num_views = num_views\n",
    "        self.render_s = render_s\n",
    "        \n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(num_views, 64, 3, 1, padding='same'),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 2),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            nn.Conv2d(128, 128, 3, 2),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            nn.Conv2d(128, 128, 3, 2),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            nn.Conv2d(128, 64, 3, 2),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.encmean = nn.Linear(64*3*3, z_dim)\n",
    "        self.encvar = nn.Linear(64*3*3, z_dim)\n",
    "        \n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(z_dim, 64*3),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(64*3, 64*3),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(64*3, 64*3),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(64*3, 64*3),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(64*3, 162*3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def _encoder(self, x):\n",
    "        x = self.enc(x)\n",
    "        mean = self.encmean(x)\n",
    "        var = F.softplus(self.encvar(x))\n",
    "        return mean, var\n",
    "    \n",
    "    def _sample_z(self, mean, var):\n",
    "        epsilon = torch.randn(mean.shape).to(device)\n",
    "        return mean + torch.sqrt(var) * epsilon\n",
    " \n",
    "    def _decoder(self, z):\n",
    "        new_verts = self.dec(z)\n",
    "        new_verts = torch.reshape(new_verts, (new_verts.shape[1]//3, 3))\n",
    "                \n",
    "        mesh = ico_sphere(2, device)\n",
    "        verts_size = mesh.verts_packed().shape[0]\n",
    "        mesh._verts_packed = new_verts\n",
    "        return self.render_s.render_silhouette(mesh)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, var = self._encoder(x)\n",
    "        z = self._sample_z(mean, var)\n",
    "        x = self._decoder(z)\n",
    "        return x  # return x, z\n",
    "    \n",
    "    def loss(self, x):\n",
    "        mean, var = self._encoder(x)\n",
    "        KL = -0.5 * torch.mean(torch.sum(1 + torch.log(var) - mean**2 - var))\n",
    "        z = self._sample_z(mean, var)\n",
    "        y = self._decoder(z)\n",
    "        \n",
    "        reconstruction = torch.mean(torch.sum(x * torch.log(y) + (1 - x) * torch.log(1 - y)))\n",
    "        lower_bound = [-KL, reconstruction]                                      \n",
    "        return -sum(lower_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b4359",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "render_s = Render_silhouette(NUM_VIEWS, device, image_size=64)\n",
    "model = VarAutoEncoder(Z_DIM, NUM_VIEWS, render_s).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=Lr)\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "import random\n",
    "import glob\n",
    "file_pathes = glob.glob('modelnet/off/*.off')\n",
    "\n",
    "from torch.autograd import detect_anomaly\n",
    "io = IO()\n",
    "\n",
    "for i in range(MAX_ITER):\n",
    "#     with detect_anomaly():\n",
    "    obj_filename = random.choice(file_pathes)\n",
    "    print(obj_filename, end=', ')\n",
    "    mesh = io.load_mesh(obj_filename, device=device, load_textures=True)\n",
    "    x = render_s.render_silhouette(mesh)\n",
    "\n",
    "    model.zero_grad()\n",
    "    y = model(x)\n",
    "    loss = model.loss(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.cpu().detach().numpy())\n",
    "    print(f'iter:{(i+1)}/{MAX_ITER}, loss:{losses[-1]}')\n",
    "\n",
    "    if (i+1) % SAVE_ITER == 0:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        for idx in range(3*5):\n",
    "            plt.subplot(6, 5, idx+1)\n",
    "            plt.imshow(x[0, idx].cpu().detach().numpy())\n",
    "            plt.axis('off')\n",
    "        for idx in range(3*5):\n",
    "            plt.subplot(6, 5, idx+1+(3*5))\n",
    "            plt.imshow(y[0, idx].cpu().detach().numpy())\n",
    "            plt.axis('off')\n",
    "        plt.savefig('result/modelnet/result_{:07d}.jpg'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d28682",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(losses)\n",
    "plt.savefig('result/modelnet/train_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb5512",
   "metadata": {},
   "source": [
    "## sampling and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af404d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    z = torch.randn(1, Z_DIM).to(device)\n",
    "    y = model._decoder(z)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for x in range(3*5):\n",
    "        plt.subplot(3, 5, x+1)\n",
    "        plt.imshow(y[0, x].cpu().detach().numpy())\n",
    "        plt.axis('off')\n",
    "    plt.savefig('result/modelnet/sample_{:07d}.jpg'.format(i+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
